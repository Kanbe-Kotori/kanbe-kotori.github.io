---
layout: post
category: notes
title: 神经病与神经网络（6）
excerpt: 从《人妻约会指南》到大语言模型幻觉
---

&emsp;&emsp;大概从一个月或是几个月前开始，《人妻约会指南》这个莫名其妙的词汇开始频频出现于我所在的各种群聊里。出于好奇（或是为了避免成为互联网原始人）的心理，我也去网上搜索了一下相关信息，结果却是和往常一样的无聊，因为这基本上就是一份由一个（我没听说过但据说很有名的）名叫李新野的“清华姚班毕业网红”所撰写的莫名其妙的“指南”。在这份“指南”中，作者自称总结了他和若干已婚女性交往的经验，并试图用进化心理学、博弈论乃至数学化语言来对这些女性的思想进行某种“拟合”或者“建模”。对此，群友**LW**认为，这背后无疑体现了作者在年轻时期遭遇情感挫折后的一种创伤补偿；群友**choom**觉得，这种“富哥感情吃扁”的叙事天然具有传播性与猎奇性，因此才会被不断讨论；而我当时的看法则更为简单粗暴，只觉得这不过是典型的“小头控制大头”的结果，所以实在没有必要为这种无聊的事构建一套系统理论；与此同时，我也始终无法理解，为何总有人把“睡过几个女的”当成某种值得炫耀的资本到处吹嘘。然而随着时间流逝，当我在一段时间之后再次无意间翻阅到这些聊天记录时，却突然意识到：我们几个人之所以会像盲人摸象一样，从同一件事中得出风马牛不相及的结论，本质上是因为我们会下意识地从各自的经验与偏好出发去思考。而我当时的判断之所以显得草率，是因为我过度聚焦于行为本身的粗糙动机，却忽略了一个关键问题：为什么这样一份私人“小头”经历的陈述，竟能呈现出一种自洽、严密、仿佛“理论体系”般的结构？这显然并不是用“小头控制大头”几个字就能敷衍过去的简单现象。

&emsp;&emsp;要解释这种“理论化的自洽”从何而来，我们需要从一个颇为反直觉的事实谈起。正如我在本系列此前的杂谈中提到过的**利贝特实验**所揭示的那样，人类所谓的“意识”可能并非决策的源头，而更像是一种事后的“感受者”，因为实验表明大脑的“行动准备信号”出现的时间早于我们主观意识到自己做出决定的时间。换言之，许多我们以为是“自己决定要做”的行为，实际上早在意识察觉之前便已经由神经系统启动；而随后迟到的“意识”则更像是一个观察者，在行为发生之后才为其补上一个看似合理的解释。

&emsp;&emsp;为了更直观地说明这一点，**“裂脑人实验”** 呈现的现象或许更为直接。所谓 **“裂脑人”**，指的是那些因治疗癫痫而接受胼胝体切断术的患者，而胼胝体作为连接左右脑半球的主要纤维束，在被切断后，患者两侧大脑便无法直接交换信息。在该系列的经典实验中，研究者向患者的右脑（左眼）展示“雪景图”，同时向左脑（右眼）展示“鸡爪图”。随后，当患者被要求从一组图片中挑选相关物品时，他的左手（由右脑控制）通常会指向雪地用的铁锹，而右手（由左脑控制）则往往会指向鸡爪所对应的鸡。然而十分有趣的是，当研究者追问“你为什么选了铁锹？”时，患者由于负责语言的左脑无法访问右脑所看到的雪景，于是只能即兴编造一个听起来合理的解释，比如：“因为你给我看了鸡爪，鸡住的地方需要清理，所以我选了铁锹。”这种“虚构但自洽”的解释显然并不符合实际，但它恰恰说明，人脑呈现给外界的叙事并非来源于其对内部因果链条的透明访问，而只是一种即兴生成的合理化机制（confabulation）。这并不是一种病理性的特征，而是大脑正常运作方式的一部分。也就是说，当信息不完整、因果链条缺损时，我们的大脑往往会本能地填补空白，而这种填补的方式（至少在自己看来）确实自洽、可信，以至于让当事人自己都深信不疑。因此，即使一个人最初的行为可能只是源于某种冲动或混乱的动机，但只要这些行为随后被整合进既有的自我叙事系统，大脑就会主动为其建构理论、假设与结构，使其在整体上看上去不仅合理，而且显得深刻而系统。

&emsp;&emsp;而当我们从这个角度去回头审视《人妻约会指南》那套貌似严密的理论框架时，便会惊讶地发现它的存在突然变得不那么怪异了。作者的初始行为可能确如群友所说，只是某种心理创伤后的补偿反应；又或者像我所判断的那样，只是由本能冲动（小头）所驱使。但随后，为了维持叙事上的稳定性，作者的大脑便会不自觉地启动解释器，将零散的经历与冲动补写成系统化的“理论”，以避免自我认知的断裂。久而久之，这种叙事便会逐渐从“为什么我当时这么做”演变成“我这样做是因为世界本来如此”，甚至进一步形成某种类似公理系统的闭合结构。这可能也解释了为什么当一个人在为自己的行为构建了新的解释框架后，往往会强烈捍卫它，并不断扩张它的适用范围，以证明自己的选择“向来合理”——而这正是大名鼎鼎的所谓的“皈依者狂热”现象。

&emsp;&emsp;人脑这种自动补全、事后生造因果链的解释过程，让我猛然间联想到了大语言模型（LLM）的“幻觉”问题。我们都知道，所谓**幻觉**（hallucination），指的是LLM在缺乏真实信息时，依然生成结构完整、语义连贯、看似合理但事实上错误的内容。而与人脑的“合理化机制”类似的是，幻觉也并非是模型本身“出了什么故障”，而是它在既定训练目标下的自然表现。从学术角度来说，大规模语言模型的预训练目标是最大化条件概率$$P(x_t∣x_{<t})$$，即在给定上下文的情况下，预测统计上最可能的token序列。然而，在这一目标中却通常并不会包含“真实性”约束，也不会要求模型判断信息是否可靠，更不会让它检验自己的推断基础。因此，当模型遭遇不完整信息或陌生领域时，它依然必须生成一个高度连贯的回答——因为训练过程中，模型既不会因为保持沉默而受到奖励，也不会因为承认“我不知道”而降低损失。与之相反，只要输出在形式上足够合理、符合语言统计结构，它就算是完成了任务。于是，当输入提供的外部硬约束不足时，模型往往会倾向于以局部语言的一致性来弥补全局知识结构的缺失，从而产生幻觉。

&emsp;&emsp;正因如此，LLM在面对不熟悉的领域时，常常表现得像一个目光清澈（什么也不会）的大学生，在期末考试时试图通过在卷面上写满连自己也不明白的公式和内容，来试图从老师手里骗到一点步骤分。而作为LLM的“判卷老师”，模型的训练损失往往也确实分辨不出这些内容的真伪，只会因为它写得“好像那么回事”而对其予以肯定，最终让它成功蒙混过关。正如裂脑人左脑在无法访问真实信息时，仍然努力编出一个“最像解释的解释”那样，LLM 也会在缺乏知识时寻找上下文中最可能的延续方式，无论这种延续是否与真实世界的因果结构一致。换言之，系统会优先满足叙事的连贯性，而非事实的正确性。从这个意义上说，幻觉绝不是 LLM 独有的“缺陷”，而是任何基于语言生成的系统在缺乏外部约束时必然呈现的自然结果。在这一点上，人脑与人工神经网络的行为几乎是同构的。

&emsp;&emsp;然而，无论是对于人类还是LLM来说，更危险的一点都在于，这种解释过程可能会反过来强化行为，从而形成一种难以察觉却极具破坏性的正反馈循环。在机器学习中，有一类典型的情形称为**半监督学习**（semi-supervised learning），指的是只有一小部分数据带有标签，而模型必须学会利用数据集中大量未标注样本的情况。在这类任务中，最常用的方法之一是使用**伪标签**（pseudo-labeling），即模型先对未标注的数据做出预测，再将自己的预测当作“标签”加入训练集，并继续更新参数。这种方法虽然听上去非常巧妙，却存在一个根本性的问题：一旦模型的早期预测出现偏差，后续的训练就只会在这个偏差上不断累积，而永远无法去纠正它。随着训练的不断迭代，模型看似逐渐“学习得更自信”，实际上却是在不断远离真实目标，最终收敛到一个完全错误却高度稳定的状态，一个被自身幻觉塑造出来的世界当中。而这种现象就叫做确认偏置（confirmation bias）。

&emsp;&emsp;在人类的认知过程中，同样的机制也会以极为相似的方式出现。一个原本来自冲动、情绪或者偶然经验的行为，只要被大脑的“解释器”包装成看似“合理”的结果，就获得了某种意义上的正当性，而这种正当性又会进一步塑造未来的决策。随着时间的推移，最初那一点模糊的动机可能就会被不断强化、修饰、系统化，最终演变成一个稳定的信念框架，甚至成为个体认知结构中不可动摇的部分。到了这一步，不仅最初的偶然性已经被完全抹平，就连行为本身的伦理或逻辑问题都可能被彻底遮蔽，因为这套叙事体系已经在不断的正反馈循环中变得越来越牢固，最终形成一种封闭的因果回路。这种机制与我在本系列之前的杂谈中所讨论的“训练数据投毒”理论几乎如出一辙：一个人的神经网络的训练过程往往高度依赖于他在成长过程中所接触到的经验、情绪反馈、社会信号等数据。而一旦这些数据由于原生家庭创伤、长期压力或错误的奖励机制等因素而出现偏差，那它们就会开始改变大脑的参数分布，使人的思想逐渐收敛到某种错误的局部最优当中。而当一个人的世界模型在这种偏置的数据环境下生成叙事时，它又会自然构建出越来越闭合的框架，将偏差进一步放大，最终形成难以修正的认知结构。至此，行为推动解释，解释又反过来巩固行为，二者彼此强化，最终形成一个稳定却错误的循环体系。

&emsp;&emsp;这种机制乍听上去确实相当让人悲观，似乎任何一个缺乏外部校准的系统，都只会沿着既有的随机偏置不断延展，最终把原本可被检视和修正的错误逐渐固化为某种不可撼动的信念——无论这个系统是由神经元构成，还是由参数矩阵构成。然而这个问题也绝非无解：对于LLM来说，人们为了抑制幻觉，常常会采用多种策略，比如通过**检索增强生成**（RAG）来为模型注入外部事实，以硬性约束其输出不至于脱离现实；或是通过**思维链**（CoT）让模型显式地展开推理步骤，从而避免在缺乏依据时直接跳到一个虚构而连贯的结论；又或者让多个模型相互比对、交叉验证，避免单一模型在局部统计结构上的偏差被不断放大。这些技术无论是引入外部信息、延展推理链条，还是增加校验环节，其共同点都在于：打破系统内部的自洽闭环。

&emsp;&emsp;而当我们把视角放回到人类自身时，便会发现我们所面临的困境其实与之并无本质不同。如果说确认偏置是一种人脑所固有的“伪标签机制”，那么对抗它的方式就不可能只依赖于内部自省，因为自省本身仍然受制于同一套错误的世界模型。要真正产生校正效应，就必须引入系统之外的东西，例如来自他人的视角、来自成熟知识体系的结构化输入、来自现实世界的反例、以及来自与自身经验相冲突的材料等等。换句话说，人类版的 RAG，就是要让自己的认知不断暴露在外部事实的冲击之下；而人类版的CoT，则是让推理过程透明化、结构化，使自己能够在推导的中间步骤中察觉矛盾，而不是事后为结论强行补写理由。只有当新的信息不断介入、当推理链可以被展开与检视时，个体的世界模型才有可能从封闭的自洽循环中被拉离出来，然后重新获得可塑性。

&emsp;&emsp;因此，要想解决人脑的“幻觉”问题，最终的答案其实朴素得近乎无趣：广泛学习，并且尽量学习正确的东西。这看起来似乎是一句废话，但它可能确实是解决确认偏置、或者说幻觉问题的唯一有效路径。在**持续学习**（continual learning）的过程中，我们最终或许并不能彻底消灭幻觉与偏置，而是只能让它们在更大的知识结构中彼此抵消，使解释系统始终有机会被新的证据重新塑形。正如RAG与CoT并不能让LLM永远不犯错一样，我们通过这种持续不断的努力所追求的，也许也只是为大脑持续引入外部世界的张力，使世界模型永远处在可更新的状态。而到了那时，我们自然也就不至于像上一篇杂谈所批判的那样，“完全沉浸在自己的世界当中”了。
